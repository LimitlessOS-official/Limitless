/*
 * Process and Thread Scheduler
 * Priority-based preemptive scheduler with SMP support
 */

#include "kernel.h"
#include "scheduler.h"
#include "process.h"
#include "interrupt.h"
#include "vmm.h"
#include "percpu.h"
#include "signal.h"

/* Legacy sched_* API block disabled: superseded by modern priority/vruntime scheduler below. */
#if 0
/* (legacy code removed) */
#endif

/*
 * Modern per-CPU priority scheduler with vruntime fairness scaffolding.
 * This replaces the legacy single circular runqueue logic above for the
 * scheduler_* API while leaving the older sched_* API untouched (to be
 * removed once callers are migrated).
 */

#define TIMESLICE_TICKS 5

typedef struct prio_queue {
    thread_t* head; /* circular doubly-linked list */
} prio_queue_t;

typedef struct runqueue {
    spinlock_t lock;
    prio_queue_t prio[THREAD_PRIO_MAX + 1]; /* one list per priority */
    thread_t* current; /* currently running on this CPU */
} runqueue_t;

static runqueue_t g_rq[KERNEL_MAX_CPUS];
static u32 g_cpu_count = 1;
percpu_sched_t g_percpu_sched[KERNEL_MAX_CPUS];

/* Global current thread tracking */
static thread_t* g_current_thread = NULL;
static spinlock_t g_current_lock;

/* Get current thread */
thread_t* thread_current(void) {
    u32 cpu = hal_cpu_id();
    return g_rq[cpu].current;
}

/* Set current thread */
void thread_set_current(thread_t* t) {
    u32 cpu = hal_cpu_id();
    g_rq[cpu].current = t;
}

/* Priority weights (simple) for vruntime scaling */
static const u32 prio_weight[] = { 1024, 512, 256, 64 }; /* HIGH..IDLE */
static const u32 BASE_WEIGHT = 1024; /* reference weight */

static void prioq_push_locked(prio_queue_t* q, thread_t* t) {
    if (!q->head) {
        q->head = t->rq_prev = t->rq_next = t;
    } else {
        thread_t* h = q->head;
        t->rq_next = h;
        t->rq_prev = h->rq_prev;
        h->rq_prev->rq_next = t;
        h->rq_prev = t;
    }
}

static void prioq_remove_locked(prio_queue_t* q, thread_t* t) {
    if (!t->rq_next || !t->rq_prev) return;
    if (t->rq_next == t && t->rq_prev == t) {
        if (q->head == t) q->head = NULL;
    } else {
        t->rq_prev->rq_next = t->rq_next;
        t->rq_next->rq_prev = t->rq_prev;
        if (q->head == t) q->head = t->rq_next;
    }
    t->rq_next = t->rq_prev = NULL;
}

static thread_t* rq_pick_next_locked(runqueue_t* rq) {
    /* Walk priorities from HIGH (0) to LOW (larger number). */
    for (int pr = THREAD_PRIO_HIGH; pr <= THREAD_PRIO_MAX; ++pr) {
        prio_queue_t* pq = &rq->prio[pr];
        if (!pq->head) continue;
        thread_t* start = pq->head;
        thread_t* t = start;
        thread_t* best = NULL;
        u64 best_vr = UINT64_MAX;
        do {
            if (t->state == THREAD_RUNNABLE) {
                if (t->vruntime < best_vr) { best_vr = t->vruntime; best = t; }
            }
            t = t->rq_next;
        } while (t && t != start);
        if (best) return best; /* choose lowest vruntime in this priority */
    }
    return NULL;
}

void scheduler_init(void) {
    g_cpu_count = hal_cpu_count();
    if (g_cpu_count == 0 || g_cpu_count > KERNEL_MAX_CPUS) g_cpu_count = 1;
    
    spinlock_init(&g_current_lock);
    
    for (u32 i = 0; i < g_cpu_count; ++i) {
        spinlock_init(&g_rq[i].lock);
        for (int p = THREAD_PRIO_HIGH; p <= THREAD_PRIO_MAX; ++p) {
            g_rq[i].prio[p].head = NULL;
        }
        g_rq[i].current = NULL;
        g_percpu_sched[i].ticks = 0;
        g_percpu_sched[i].context_switch = 0;
    }
    
    /* Set up timer for preemption */
    hal_timer_set_periodic(100);  /* 100 Hz timer for scheduler ticks */
}

/* Timer tick: account runtime, enforce timeslice */
void scheduler_tick(void) {
    u32 cpu = hal_cpu_id();
    runqueue_t* rq = &g_rq[cpu];
    thread_t* cur = rq->current;
    if (!cur) return;
    if (cur->state != THREAD_RUNNING) return;

    g_percpu_sched[cpu].ticks++;

    /* vruntime increment scaled inversely by weight */
    u32 pr = (cur->priority < THREAD_PRIO_HIGH || cur->priority > THREAD_PRIO_MAX) ? THREAD_PRIO_NORMAL : cur->priority;
    u32 w = prio_weight[pr];
    if (w == 0) w = BASE_WEIGHT; /* safety */
    /* Add scaled value (higher priority => larger weight => smaller vruntime advance) */
    cur->vruntime += (BASE_WEIGHT / w);

    if (++cur->ticks_used >= TIMESLICE_TICKS) {
        cur->need_resched = 1;
        interrupt_request_reschedule();
    }
}

void scheduler_enqueue(thread_t* t) {
    u32 cpu = t->affinity_cpu < g_cpu_count ? t->affinity_cpu : (t->affinity_cpu % g_cpu_count);
    runqueue_t* rq = &g_rq[cpu];
    spin_lock(&rq->lock);
    if (t->priority < THREAD_PRIO_HIGH) t->priority = THREAD_PRIO_HIGH;
    if (t->priority > THREAD_PRIO_MAX) t->priority = THREAD_PRIO_MAX;
    if (t->state != THREAD_RUNNABLE) t->state = THREAD_RUNNABLE;
    if (!t->on_rq) {
        prioq_push_locked(&rq->prio[t->priority], t);
        t->on_rq = 1;
    }
    spin_unlock(&rq->lock);
}

void scheduler_dequeue(thread_t* t) {
    u32 cpu = t->affinity_cpu < g_cpu_count ? t->affinity_cpu : (t->affinity_cpu % g_cpu_count);
    runqueue_t* rq = &g_rq[cpu];
    spin_lock(&rq->lock);
    if (t->on_rq) {
        prioq_remove_locked(&rq->prio[t->priority], t);
        t->on_rq = 0;
    }
    spin_unlock(&rq->lock);
}

void scheduler_yield(void) {
    u32 cpu = hal_cpu_id();
    runqueue_t* rq = &g_rq[cpu];
    spin_lock(&rq->lock);
    thread_t* cur = rq->current;
    if (cur && cur->state == THREAD_RUNNING) {
        cur->need_resched = 1;
    }
    spin_unlock(&rq->lock);
    scheduler_schedule();
}

void scheduler_schedule(void) {
    hal_interrupt_disable();
    u32 cpu = hal_cpu_id();
    runqueue_t* rq = &g_rq[cpu];

    spin_lock(&rq->lock);
    thread_t* cur = rq->current;
    if (cur && cur->state == THREAD_RUNNING && cur->need_resched) {
        cur->need_resched = 0;
        cur->ticks_used = 0;
        cur->state = THREAD_RUNNABLE;
        if (!cur->on_rq) {
            prioq_push_locked(&rq->prio[cur->priority], cur);
            cur->on_rq = 1;
        }
    }

    thread_t* next = rq_pick_next_locked(rq);
    if (!next) {
        if (cur) { /* keep running current */
            cur->state = THREAD_RUNNING;
            cur->need_resched = 0;
            spin_unlock(&rq->lock);
            hal_interrupt_enable();
            return;
        }
        spin_unlock(&rq->lock);
        hal_interrupt_enable();
        return;
    }

    if (next->on_rq) {
        prioq_remove_locked(&rq->prio[next->priority], next);
        next->on_rq = 0;
    }
    next->state = THREAD_RUNNING;
    thread_t* prev = cur;
    rq->current = next;
    spin_unlock(&rq->lock);

    if (prev != next) {
        g_percpu_sched[cpu].context_switch++;
    }

    if (prev == next) {
        hal_interrupt_enable();
        return;
    }

    struct arch_context** old_ctx = prev ? &prev->arch_ctx : NULL;
    struct arch_context* new_ctx = next->arch_ctx;
    hal_arch_switch_context(old_ctx, new_ctx);
    
    /* Deliver pending signals to the newly scheduled process */
    if (next && next->proc) {
        signal_deliver_pending(next->proc);
    }
    
    hal_interrupt_enable();
}

int scheduler_create_kthread(thread_t** out_thread, void (*entry)(void*), void* arg, void* stack_base, size_t stack_size, u32 affinity_cpu) {
    if (!out_thread || !entry || !stack_base || stack_size < 4096) return K_EINVAL;
    thread_t* t = process_alloc_kernel_thread(entry, arg, stack_base, stack_size);
    if (!t) return K_ENOMEM;
    t->affinity_cpu = affinity_cpu % g_cpu_count;
    t->state = THREAD_RUNNABLE;
    t->ticks_used = 0;
    t->need_resched = 0;
    t->on_rq = 0;
    t->rq_next = t->rq_prev = NULL;
    /* default priority NORMAL if unset */
    if (t->priority < THREAD_PRIO_HIGH || t->priority > THREAD_PRIO_MAX) t->priority = THREAD_PRIO_NORMAL;
    scheduler_enqueue(t);
    *out_thread = t;
    return K_OK;
}

void scheduler_wake(thread_t* t) {
    if (!t) return;
    if (t->state == THREAD_BLOCKED || t->state == THREAD_SLEEPING) {
        t->state = THREAD_RUNNABLE;
        scheduler_enqueue(t);
    }
}

/* Put current thread to sleep */
void scheduler_sleep(thread_t* t) {
    if (!t) return;
    
    t->state = THREAD_SLEEPING;
    scheduler_dequeue(t);
    scheduler_schedule();  /* Switch to next thread */
}

/* Block current thread (for I/O, locks, etc.) */
void scheduler_block(thread_t* t) {
    if (!t) return;
    
    t->state = THREAD_BLOCKED;
    scheduler_dequeue(t);
    scheduler_schedule();  /* Switch to next thread */
}

/* Idle thread that runs when no other threads are runnable */
static void idle_thread_func(void* arg) {
    (void)arg;
    
    while (1) {
        hal_cpu_halt();  /* Halt CPU until interrupt */
        scheduler_yield();
    }
}

/* Create system idle thread */
int scheduler_create_idle_thread(void) {
    /* Allocate stack for idle thread */
    void* stack = vmm_kmalloc(8192, 16);
    if (!stack) return K_ENOMEM;
    
    thread_t* idle = NULL;
    int result = scheduler_create_kthread(&idle, idle_thread_func, NULL, stack, 8192, 0);
    if (result != K_OK) {
        vmm_kfree(stack, 8192);
        return result;
    }
    
    idle->priority = THREAD_PRIO_IDLE;
    return K_OK;
}

/* Put current thread to sleep */
void scheduler_sleep(thread_t* t) {
    if (!t) return;
    
    t->state = THREAD_SLEEPING;
    scheduler_dequeue(t);
    scheduler_schedule();  /* Switch to next thread */
}

/* Block current thread (for I/O, locks, etc.) */
void scheduler_block(thread_t* t) {
    if (!t) return;
    
    t->state = THREAD_BLOCKED;
    scheduler_dequeue(t);
    scheduler_schedule();  /* Switch to next thread */
}

/* Idle thread that runs when no other threads are runnable */
static void idle_thread_func(void* arg) {
    (void)arg;
    
    while (1) {
        hal_cpu_halt();  /* Halt CPU until interrupt */
        scheduler_yield();
    }
}

/* Create system idle thread */
int scheduler_create_idle_thread(void) {
    /* Allocate stack for idle thread */
    void* stack = vmm_kmalloc(8192, 16);
    if (!stack) return K_ENOMEM;
    
    thread_t* idle = NULL;
    int result = scheduler_create_kthread(&idle, idle_thread_func, NULL, stack, 8192, 0);
    if (result != K_OK) {
        vmm_kfree(stack, 8192);
        return result;
    }
    
    idle->priority = THREAD_PRIO_IDLE;
    return K_OK;
}
